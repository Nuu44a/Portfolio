{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HomeWork02_8weeks_chatbot.ipynb","provenance":[{"file_id":"18YqFl_6P3jlsd2Aq6C9gy9gWUJ7yxA2x","timestamp":1619069293159}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4i5b0ZSl7v8","executionInfo":{"status":"ok","timestamp":1619379186696,"user_tz":-180,"elapsed":15188,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"bba14d80-37be-495b-cde3-b6ea22d5f94c"},"source":["!pip install nltk\n","!pip install pymorphy2\n","!pip install pymorphy2-dicts"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Collecting pymorphy2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n","\u001b[?25hCollecting dawg-python>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Collecting pymorphy2-dicts-ru<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n","Installing collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n","Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n","Collecting pymorphy2-dicts\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 4.4MB/s \n","\u001b[?25hInstalling collected packages: pymorphy2-dicts\n","Successfully installed pymorphy2-dicts-2.4.393442.3710985\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oVvW_X5qmfgR","executionInfo":{"status":"ok","timestamp":1619379193675,"user_tz":-180,"elapsed":2524,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["import nltk\n","import numpy as np\n","import random\n","import string"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpNHG0-2mfwV","executionInfo":{"status":"ok","timestamp":1619379203317,"user_tz":-180,"elapsed":3654,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["# with open('chatbot.txt','r',errors = 'ignore') as f:\n","  # raw=f.read()\n","# Текст взят из статьи по адресу: https://vas3k.ru/blog/machine_learning/#scroll60\n","raw = '''\n","Машинное обучение для людей\n","Разбираемся простыми словами\n","Машинное обучение — как секс в старших классах. Все говорят о нем по углам, единицы понимают, а занимается только препод. Статьи о машинном обучении делятся на два типа: это либо трёхтомники с формулами и теоремами, которые я ни разу не смог дочитать даже до середины, либо сказки об искусственном интеллекте, профессиях будущего и волшебных дата-саентистах.\n","Зачем обучать машины\n","Снова разберём на Олегах.\n","Предположим, Олег хочет купить автомобиль и считает сколько денег ему нужно для этого накопить. Он пересмотрел десяток объявлений в интернете и увидел, что новые автомобили стоят около $20 000, годовалые — примерно $19 000, двухлетние — $18 000 и так далее.\n","В уме Олег-аналитик выводит формулу: адекватная цена автомобиля начинается от $20 000 и падает на $1000 каждый год, пока не упрётся в $10 000.\n","Олег сделал то, что в машинном обучении называют регрессией — предсказал цену по известным данным. Люди делают это постоянно, когда считают почём продать старый айфон или сколько шашлыка взять на дачу (моя формула — полкило на человека в сутки).\n","Да, было бы удобно иметь формулу под каждую проблему на свете. Но взять те же цены на автомобили: кроме пробега есть десятки комплектаций, разное техническое состояние, сезонность спроса и еще столько неочевидных факторов, которые Олег, даже при всём желании, не учел бы в голове.\n","Люди тупы и ленивы — надо заставить вкалывать роботов. Пусть машина посмотрит на наши данные, найдёт в них закономерности и научится предсказывать для нас ответ. Самое интересное, что в итоге она стала находить даже такие закономерности, о которых люди не догадывались.\n","Так родилось машинное обучение.\n","Три составляющие обучения\n","Цель машинного обучения — предсказать результат по входным данным. Чем разнообразнее входные данные, тем проще машине найти закономерности и тем точнее результат.\n","Итак, если мы хотим обучить машину, нам нужны три вещи:\n","Данные Хотим определять спам — нужны примеры спам-писем, предсказывать курс акций — нужна история цен, узнать интересы пользователя — нужны его лайки или посты. Данных нужно как можно больше. Десятки тысяч примеров — это самый злой минимум для отчаянных.\n","Данные собирают как могут. Кто-то вручную — получается дольше, меньше, зато без ошибок. Кто-то автоматически — просто сливает машине всё, что нашлось, и верит в лучшее. Самые хитрые, типа гугла, используют своих же пользователей для бесплатной разметки. Вспомните ReCaptcha, которая иногда требует «найти на фотографии все дорожные знаки» — это оно и есть.\n","За хорошими наборами данных (датасетами) идёт большая охота. Крупные компании, бывает, раскрывают свои алгоритмы, но датасеты — крайне редко.\n","Признаки Мы называем их фичами (features), так что ненавистникам англицизмов придётся страдать. Фичи, свойства, характеристики, признаки — ими могут быть пробег автомобиля, пол пользователя, цена акций, даже счетчик частоты появления слова в тексте может быть фичей.\n","Машина должна знать, на что ей конкретно смотреть. Хорошо, когда данные просто лежат в табличках — названия их колонок и есть фичи. А если у нас сто гигабайт картинок с котами? Когда признаков много, модель работает медленно и неэффективно. Зачастую отбор правильных фич занимает больше времени, чем всё остальное обучение. Но бывают и обратные ситуации, когда кожаный мешок сам решает отобрать только «правильные» на его взгляд признаки и вносит в модель субъективность — она начинает дико врать.\n","Алгоритм Одну задачу можно решить разными методами примерно всегда. От выбора метода зависит точность, скорость работы и размер готовой модели. Но есть один нюанс: если данные говно, даже самый лучший алгоритм не поможет. Не зацикливайтесь на процентах, лучше соберите побольше данных.\n","Обучение vs Интеллект\n","Однажды в одном хипстерском издании я видел статью под заголовком «Заменят ли нейросети машинное обучение». Пиарщики в своих пресс-релизах обзывают «искусственным интеллектом» любую линейную регрессию, с которой уже дети во дворе играют. Объясняю разницу на картинке, раз и навсегда.\n","Искусственный интеллект — название всей области, как биология или химия.\n","Машинное обучение — это раздел искусственного интеллекта. Важный, но не единственный.\n","Нейросети — один из видов машинного обучения. Популярный, но есть и другие, не хуже.\n","Глубокое обучение — архитектура нейросетей, один из подходов к их построению и обучению. На практике сегодня мало кто отличает, где глубокие нейросети, а где не очень. Говорят название конкретной сети и всё.\n","Сравнивать можно только вещи одного уровня, иначе получается полный буллщит типа «что лучше: машина или колесо?» Не отождествляйте термины без причины, чтобы не выглядеть дурачком.\n","Вот что машины сегодня умеют, а что не под силу даже самым обученным.\n","Машина может: Предсказывать Запоминать Воспроизводить Выбирать лучшее\n","Машина не может: Создавать новое Резко поумнеть\tВыйти за рамки задачи Убить всех людей\n","Думаю потом нарисовать полноценную настенную карту со стрелочками и объяснениями, что где используется, если статья зайдёт.\n","И да. Классифицировать алгоритмы можно десятком способов. Я выбрал этот, потому что он мне кажется самым удобным для повествования. Надо понимать, что не бывает так, чтобы задачу решал только один метод. Я буду упоминать известные примеры применений, но держите в уме, что «сын маминой подруги» всё это может решить нейросетями.\n","Начну с базового обзора. Сегодня в машинном обучении есть всего четыре основных направления.\n","Часть 1. Классическое обучение\n","Первые алгоритмы пришли к нам из чистой статистики еще в 1950-х. Они решали формальные задачи — искали закономерности в циферках, оценивали близость точек в пространстве и вычисляли направления.\n","Сегодня на классических алгоритмах держится добрая половина интернета. Когда вы встречаете блок «Рекомендованные статьи» на сайте, или банк блокирует все ваши деньги на карточке после первой же покупки кофе за границей — это почти всегда дело рук одного из этих алгоритмов.\n","Да, крупные корпорации любят решать все проблемы нейросетями. Потому что лишние 2% точности для них легко конвертируются в дополнительные 2 миллиарда прибыли. Остальным же стоит включать голову. Когда задача решаема классическими методами, дешевле реализовать сколько-нибудь полезную для бизнеса систему на них, а потом думать об улучшениях. А если вы не решили задачу, то не решить её на 2% лучше вам не особо поможет.\n","Знаю несколько смешных историй, когда команда три месяца переписывала систему рекомендаций интернет-магазина на более точный алгоритм, и только потом понимала, что покупатели вообще ей не пользуются. Большая часть просто приходит из поисковиков.\n","При всей своей популярности, классические алгоритмы настолько просты, что их легко объяснить даже ребёнку. Сегодня они как основы арифметики — пригождаются постоянно, но некоторые всё равно стали их забывать.\n","Обучение с учителем\n","Классическое обучение любят делить на две категории — с учителем и без. Часто можно встретить их английские наименования — Supervised и Unsupervised Learning.\n","В первом случае у машины есть некий учитель, который говорит ей как правильно. Рассказывает, что на этой картинке кошка, а на этой собака. То есть учитель уже заранее разделил (разметил) все данные на кошек и собак, а машина учится на конкретных примерах.\n","В обучении без учителя, машине просто вываливают кучу фотографий животных на стол и говорят «разберись, кто здесь на кого похож». Данные не размечены, у машины нет учителя, и она пытается сама найти любые закономерности. Об этих методах поговорим ниже.\n","Очевидно, что с учителем машина обучится быстрее и точнее, потому в боевых задачах его используют намного чаще. Эти задачи делятся на два типа: классификация — предсказание категории объекта, и регрессия — предсказание места на числовой прямой.\n","Классификация\n","«Разделяет объекты по заранее известному признаку. Носки по цветам, документы по языкам, музыку по жанрам»\n","Сегодня используют для:\n","Спам-фильтры\n","Определение языка\n","Поиск похожих документов\n","Анализ тональности\n","Распознавание рукописных букв и цифр\n","Определение подозрительных транзакций\n","Популярные алгоритмы: Наивный Байес, Деревья Решений, Логистическая Регрессия, K-ближайших соседей, Машины Опорных Векторов\n","Здесь и далее в комментах можно дополнять эти блоки. Приводите свои примеры задач, областей и алгоритмов, потому что описанные мной взяты из субъективного опыта.\n","Классификация вещей — самая популярная задача во всём машинном обучении. Машина в ней как ребёнок, который учится раскладывать игрушки: роботов в один ящик, танки в другой. Опа, а если это робот-танк? Штош, время расплакаться и выпасть в ошибку.\n"," Старый доклад Бобука про повышение конверсии лендингов с помощью SVM\n","Для классификации всегда нужен учитель — размеченные данные с признаками и категориями, которые машина будет учиться определять по этим признакам. Дальше классифицировать можно что угодно: пользователей по интересам — так делают алгоритмические ленты, статьи по языкам и тематикам — важно для поисковиков, музыку по жанрам — вспомните плейлисты Спотифая и Яндекс.Музыки, даже письма в вашем почтовом ящике.\n","Раньше все спам-фильтры работали на алгоритме Наивного Байеса. Машина считала сколько раз слово «виагра» встречается в спаме, а сколько раз в нормальных письмах. Перемножала эти две вероятности по формуле Байеса, складывала результаты всех слов и бац, всем лежать, у нас машинное обучение!\n","Позже спамеры научились обходить фильтр Байеса, просто вставляя в конец письма много слов с «хорошими» рейтингами. Метод получил ироничное название Отравление Байеса, а фильтровать спам стали другими алгоритмами. Но метод навсегда остался в учебниках как самый простой, красивый и один из первых практически полезных.\n","Возьмем другой пример полезной классификации. Вот берёте вы кредит в банке. Как банку удостовериться, вернёте вы его или нет? Точно никак, но у банка есть тысячи профилей других людей, которые уже брали кредит до вас. Там указан их возраст, образование, должность, уровень зарплаты и главное — кто из них вернул кредит, а с кем возникли проблемы.\n","Да, все догадались, где здесь данные и какой надо предсказать результат. Обучим машину, найдём закономерности, получим ответ — вопрос не в этом. Проблема в том, что банк не может слепо доверять ответу машины, без объяснений. Вдруг сбой, злые хакеры или бухой админ решил скриптик исправить.\n","Для этой задачи придумали Деревья Решений. Машина автоматически разделяет все данные по вопросам, ответы на которые «да» или «нет». Вопросы могут быть не совсем адекватными с точки зрения человека, например «зарплата заёмщика больше, чем 25934 рубля?», но машина придумывает их так, чтобы на каждом шаге разбиение было самым точным.\n","Так получается дерево вопросов. Чем выше уровень, тем более общий вопрос. Потом даже можно загнать их аналитикам, и они навыдумывают почему так.\n","Деревья нашли свою нишу в областях с высокой ответственностью: диагностике, медицине, финансах.\n","Два самых популярных алгоритма построения деревьев — CART и C4.5.\n","В чистом виде деревья сегодня используют редко, но вот их ансамбли (о которых будет ниже) лежат в основе крупных систем и зачастую уделывают даже нейросети. Например, когда вы задаете вопрос Яндексу, именно толпа глупых деревьев бежит ранжировать вам результаты.\n","Но самым популярным методом классической классификации заслуженно является Метод Опорных Векторов (SVM). Им классифицировали уже всё: виды растений, лица на фотографиях, документы по тематикам, даже странных Playboy-моделей. Много лет он был главным ответом на вопрос «какой бы мне взять классификатор».\n","Идея SVM по своей сути проста — он ищет, как так провести две прямые между категориями, чтобы между ними образовался наибольший зазор. На картинке видно нагляднее:\n","У классификации есть полезная обратная сторона — поиск аномалий. Когда какой-то признак объекта сильно не вписывается в наши классы, мы ярко подсвечиваем его на экране. Сейчас так делают в медицине: компьютер подсвечивает врачу все подозрительные области МРТ или выделяет отклонения в анализах. На биржах таким же образом определяют нестандартных игроков, которые скорее всего являются инсайдерами. Научив компьютер «как правильно», мы автоматически получаем и обратный классификатор — как неправильно.\n","Сегодня для классификации всё чаще используют нейросети, ведь по сути их для этого и изобрели.\n","Правило буравчика такое: сложнее данные — сложнее алгоритм. Для текста, цифр, табличек я бы начинал с классики. Там модели меньше, обучаются быстрее и работают понятнее. Для картинок, видео и другой непонятной бигдаты — сразу смотрел бы в сторону нейросетей.\n","Лет пять назад еще можно было встретить классификатор лиц на SVM, но сегодня под эту задачу сотня готовых сеток по интернету валяются, чо бы их не взять. А вот спам-фильтры как на SVM писали, так и не вижу смысла останавливаться.\n","Регрессия\n","«Нарисуй линию вдоль моих точек. Да, это машинное обучение»\n","Сегодня используют для:\n","Прогноз стоимости ценных бумаг\n","Анализ спроса, объема продаж\n","Медицинские диагнозы\n","Любые зависимости числа от времени\n","Популярные алгоритмы: Линейная или Полиномиальная Регрессия\n","Регрессия — та же классификация, только вместо категории мы предсказываем число. Стоимость автомобиля по его пробегу, количество пробок по времени суток, объем спроса на товар от роста компании и.т.д. На регрессию идеально ложатся любые задачи, где есть зависимость от времени.\n","Регрессию очень любят финансисты и аналитики, она встроена даже в Excel. Внутри всё работает, опять же, банально: машина тупо пытается нарисовать линию, которая в среднем отражает зависимость. Правда, в отличии от человека с фломастером и вайтбордом, делает она это математически точно — считая среднее расстояние до каждой точки и пытаясь всем угодить.\n","Когда регрессия рисует прямую линию, её называют линейной, когда кривую — полиномиальной. Это два основных вида регрессии, дальше уже начинаются редкоземельные методы. Но так как в семье не без урода, есть Логистическая Регрессия, которая на самом деле не регрессия, а метод классификации, от чего у всех постоянно путаница. Не делайте так.\n","Схожесть регрессии и классификации подтверждается еще и тем, что многие классификаторы, после небольшого тюнинга, превращаются в регрессоры. Например, мы можем не просто смотреть к какому классу принадлежит объект, а запоминать, насколько он близок — и вот, у нас регрессия.\n","Для желающих понять это глубже, но тоже простыми словами, рекомендую цикл статей Machine Learning for Humans\n","Обучение без учителя\n","Обучение без учителя (Unsupervised Learning) было изобретено позже, аж в 90-е, и на практике используется реже. Но бывают задачи, где у нас просто нет выбора.\n","Размеченные данные, как я сказал, дорогая редкость. Но что делать если я хочу, например, написать классификатор автобусов — идти на улицу руками фотографировать миллион сраных икарусов и подписывать где какой? Так и жизнь вся пройдёт, а у меня еще игры в стиме не пройдены.\n","Когда нет разметки, есть надежда на капитализм, социальное расслоение и миллион китайцев из сервисов типа Яндекс.Толока, которые готовы делать для вас что угодно за пять центов. Так обычно и поступают на практике. А вы думали где Яндекс берёт все свои крутые датасеты?\n","Либо, можно попробовать обучение без учителя. Хотя, честно говоря, из своей практики я не помню чтобы где-то оно сработало хорошо.\n","Обучение без учителя, всё же, чаще используют как метод анализа данных, а не как основной алгоритм. Специальный кожаный мешок с дипломом МГУ вбрасывает туда кучу мусора и наблюдает. Кластеры есть? Зависимости появились? Нет? Ну штош, продолжай, труд освобождает. Тыж хотел работать в датасаенсе.\n","Кластеризация\n","«Разделяет объекты по неизвестному признаку. Машина сама решает как лучше»\n","Сегодня используют для:\n","Сегментация рынка (типов покупателей, лояльности)\n","Объединение близких точек на карте\n","Сжатие изображений\n","Анализ и разметки новых данных\n","Детекторы аномального поведения\n","Популярные алгоритмы: Метод K-средних, Mean-Shift, DBSCAN\n"," 2 комментария\n","Кластеризация — это классификация, но без заранее известных классов. Она сама ищет похожие объекты и объединяет их в кластеры. Количество кластеров можно задать заранее или доверить это машине. Похожесть объектов машина определяет по тем признакам, которые мы ей разметили — у кого много схожих характеристик, тех давай в один класс.\n","Отличный пример кластеризации — маркеры на картах в вебе. Когда вы ищете все крафтовые бары в Москве, движку приходится группировать их в кружочки с циферкой, иначе браузер зависнет в потугах нарисовать миллион маркеров.\n","Более сложные примеры кластеризации можно вспомнить в приложениях iPhoto или Google Photos, которые находят лица людей на фотографиях и группируют их в альбомы. Приложение не знает как зовут ваших друзей, но может отличить их по характерным чертам лица. Типичная кластеризация.\n","Правда для начала им приходится найти эти самые «характерные черты», а это уже только с учителем.\n","Сжатие изображений — еще одна популярная проблема. Сохраняя картинку в PNG, вы можете установить палитру, скажем, в 32 цвета. Тогда кластеризация найдёт все «примерно красные» пиксели изображения, высчитает из них «средний красный по больнице» и заменит все красные на него. Меньше цветов — меньше файл.\n","Проблема только, как быть с цветами типа Cyan ?? — вот он ближе к зеленому или синему? Тут нам поможет популярный алгоритм кластеризации — Метод К-средних (K-Means). Мы случайным образом бросаем на палитру цветов наши 32 точки, обзывая их центроидами. Все остальные точки относим к ближайшему центроиду от них — получаются как бы созвездия из самых близких цветов. Затем двигаем центроид в центр своего созвездия и повторяем пока центроиды не перестанут двигаться. Кластеры обнаружены, стабильны и их ровно 32 как и надо было.\n","Искать центроиды удобно и просто, но в реальных задачах кластеры могут быть совсем не круглой формы. Вот вы геолог, которому нужно найти на карте схожие по структуре горные породы — ваши кластеры не только будут вложены друг в друга, но вы ещё и не знаете сколько их вообще получится.\n","Хитрым задачам — хитрые методы. DBSCAN, например. Он сам находит скопления точек и строит вокруг кластеры. Его легко понять, если представить, что точки — это люди на площади. Находим трёх любых близко стоящих человека и говорим им взяться за руки. Затем они начинают брать за руку тех, до кого могут дотянуться. Так по цепочке, пока никто больше не сможет взять кого-то за руку — это и будет первый кластер. Повторяем, пока не поделим всех. Те, кому вообще некого брать за руку — это выбросы, аномалии. В динамике выглядит довольно красиво:\n","Интересующимся кластеризацией рекомендую статью The 5 Clustering Algorithms Data Scientists Need to Know\n","Как и классификация, кластеризация тоже может использоваться как детектор аномалий. Поведение пользователя после регистрации резко отличается от нормального? Заблокировать его и создать тикет саппорту, чтобы проверили бот это или нет. При этом нам даже не надо знать, что есть «нормальное поведение» — мы просто выгружаем все действия пользователей в модель, и пусть машина сама разбирается кто тут нормальный.\n","Работает такой подход, по сравнению с классификацией, не очень. Но за спрос не бьют, вдруг получится.\n","Уменьшение Размерности (Обобщение)\n","«Собирает конкретные признаки в абстракции более высокого уровня»\n","Сегодня используют для:\n","Рекомендательные Системы (?)\n","Красивые визуализации\n","Определение тематики и поиска похожих документов\n","Анализ фейковых изображений\n","Риск-менеджмент\n","Популярные алгоритмы: Метод главных компонент (PCA), Сингулярное разложение (SVD), Латентное размещение Дирихле (LDA), Латентно-семантический анализ (LSA, pLSA, GLSA), t-SNE (для визуализации)\n","Изначально это были методы хардкорных Data Scientist'ов, которым сгружали две фуры цифр и говорили найти там что-нибудь интересное. Когда просто строить графики в экселе уже не помогало, они придумали напрячь машины искать закономерности вместо них. Так у них появились методы, которые назвали Dimension Reduction или Feature Learning.\n","Проецируем 2D-данные на прямую (PCA)\n","Для нас практическая польза их методов в том, что мы можем объединить несколько признаков в один и получить абстракцию. Например, собаки с треугольными ушами, длинными носами и большими хвостами соединяются в полезную абстракцию «овчарки». Да, мы теряем информацию о конкретных овчарках, но новая абстракция всяко полезнее этих лишних деталей. Плюс, обучение на меньшем количестве размерностей идёт сильно быстрее.\n","Инструмент на удивление хорошо подошел для определения тематик текстов (Topic Modelling). Мы смогли абстрагироваться от конкретных слов до уровня смыслов даже без привлечения учителя со списком категорий. Алгоритм назвали Латентно-семантический анализ (LSA), и его идея была в том, что частота появления слова в тексте зависит от его тематики: в научных статьях больше технических терминов, в новостях о политике — имён политиков. Да, мы могли бы просто взять все слова из статей и кластеризовать, как мы делали с ларьками выше, но тогда мы бы потеряли все полезные связи между словами, например, что батарейка и аккумулятор, означают одно и то же в разных документах.\n","Точность такой системы — полное дно, даже не пытайтесь.\n","Нужно как-то объединить слова и документы в один признак, чтобы не терять эти скрытые (латентные) связи. Отсюда и появилось название метода. Оказалось, что Сингулярное разложение (SVD) легко справляется с этой задачей, выявляя для нас полезные тематические кластеры из слов, которые встречаются вместе.\n","Для понимания рекомендую статью Как уменьшить количество измерений и извлечь из этого пользу, а практическое применение хорошо описано в статье Алгоритм LSA для поиска похожих документов.\n","Другое мега-популярное применение метода уменьшения размерности нашли в рекомендательных системах и коллаборативной фильтрации (у меня был пост про их виды). Оказалось, если абстрагировать ими оценки пользователей фильмам, получается неплохая система рекомендаций кино, музыки, игр и чего угодно вообще.\n","Полученная абстракция будет с трудом понимаема мозгом, но когда исследователи начали пристально рассматривать новые признаки, они обнаружили, что какие-то из них явно коррелируют с возрастом пользователя (дети чаще играли в Майнкрафт и смотрели мультфильмы), другие с определёнными жанрами кино, а третьи вообще с синдромом поиска глубокого смысла.\n","Машина, не знавшая ничего кроме оценок пользователей, смогла добраться до таких высоких материй, даже не понимая их. Достойно. Дальше можно проводить соцопросы и писать дипломные работы о том, почему бородатые мужики любят дегенеративные мультики.\n","На эту тему есть неплохая лекция Яндекса — Как работают рекомендательные системы\n","Поиск правил (ассоциация)\n","«Ищет закономерности в потоке заказов»\n","Сегодня используют для:\n","Прогноз акций и распродаж\n","Анализ товаров, покупаемых вместе\n","Расстановка товаров на полках\n","Анализ паттернов поведения на веб-сайтах\n","Популярные алгоритмы: Apriori, Euclat, FP-growth\n","Сюда входят все методы анализа продуктовых корзин, стратегий маркетинга и других последовательностей.\n","Предположим, покупатель берёт в дальнем углу магазина пиво и идёт на кассу. Стоит ли ставить на его пути орешки? Часто ли люди берут их вместе? Орешки с пивом, наверное да, но какие ещё товары покупают вместе? Когда вы владелец сети гипермаркетов, ответ для вас не всегда очевиден, но одно тактическое улучшение в расстановке товаров может принести хорошую прибыль.\n","То же касается интернет-магазинов, где задача еще интереснее — за каким товаром покупатель вернётся в следующий раз?\n","По непонятным мне причинам, поиск правил — самая хреново продуманная категория среди всех методов обучения. Классические способы заключаются в тупом переборе пар всех купленных товаров с помощью деревьев или множеств. Сами алгоритмы работают наполовину — могут искать закономерности, но не умеют обобщать или воспроизводить их на новых примерах.\n","В реальности каждый крупный ритейлер пилит свой велосипед, и никаких особых прорывов в этой области я не встречал. Максимальный уровень технологий здесь — запилить систему рекомендаций, как в пункте выше. Хотя может я просто далёк от этой области, расскажите в комментах, кто шарит?\n","Часть 2. Обучение с подкреплением\n","«Брось робота в лабиринт и пусть ищет выход»\n","Сегодня используют для:\n","Самоуправляемых автомобилей\n","Роботов пылесосов\n","Игр\n","Автоматической торговли\n","Управления ресурсами предприятий\n","Популярные алгоритмы: Q-Learning, SARSA, DQN, A3C, Генетический Алгоритм\n","Наконец мы дошли до вещей, которые, вроде, выглядят как настоящий искусственный интеллект. Многие авторы почему-то ставят обучение с подкреплением где-то между обучением с учителем и без, но я не понимаю чем они похожи. Названием?\n","Обучение с подкреплением используют там, где задачей стоит не анализ данных, а выживание в реальной среде.\n"," Нейросеть играет в Марио\n","Средой может быть даже видеоигра. Роботы, играющие в Марио, были популярны еще лет пять назад. Средой может быть реальный мир. Как пример — автопилот Теслы, который учится не сбивать пешеходов, или роботы-пылесосы, главная задача которых — напугать вашего кота до усрачки с максимальной эффективностью.\n","Знания об окружающем мире такому роботу могут быть полезны, но чисто для справки. Не важно сколько данных он соберёт, у него всё равно не получится предусмотреть все ситуации. Потому его цель — минимизировать ошибки, а не рассчитать все ходы. Робот учится выживать в пространстве с максимальной выгодой: собранными монетками в Марио, временем поездки в Тесле или количеством убитых кожаных мешков хихихих.\n","Выживание в среде и есть идея обучения с подкреплением. Давайте бросим бедного робота в реальную жизнь, будем штрафовать его за ошибки и награждать за правильные поступки. На людях норм работает, почему бы на и роботах не попробовать.\n","Умные модели роботов-пылесосов и самоуправляемые автомобили обучаются именно так: им создают виртуальный город (часто на основе карт настоящих городов), населяют случайными пешеходами и отправляют учиться никого там не убивать. Когда робот начинает хорошо себя чувствовать в искусственном GTA, его выпускают тестировать на реальные улицы.\n","Запоминать сам город машине не нужно — такой подход называется Model-Free. Конечно, тут есть и классический Model-Based, но в нём нашей машине пришлось бы запоминать модель всей планеты, всех возможных ситуаций на всех перекрёстках мира. Такое просто не работает. В обучении с подкреплением машина не запоминает каждое движение, а пытается обобщить ситуации, чтобы выходить из них с максимальной выгодой.\n","Помните новость пару лет назад, когда машина обыграла человека в Го? Хотя незадолго до этого было доказано, что число комбинаций физически невозможно просчитать, ведь оно превышает количество атомов во вселенной. То есть если в шахматах машина реально просчитывала все будущие комбинации и побеждала, с Го так не прокатывало. Поэтому она просто выбирала наилучший выход из каждой ситуации и делала это достаточно точно, чтобы обыграть кожаного ублюдка.\n","Эта идея лежит в основе алгоритма Q-learning и его производных (SARSA и DQN). Буква Q в названии означает слово Quality, то есть робот учится поступать наиболее качественно в любой ситуации, а все ситуации он запоминает как простой марковский процесс.\n","Машина прогоняет миллионы симуляций в среде, запоминая все сложившиеся ситуации и выходы из них, которые принесли максимальное вознаграждение. Но как понять, когда у нас сложилась известная ситуация, а когда абсолютно новая? Вот самоуправляемый автомобиль стоит у перекрестка и загорается зелёный — значит можно ехать? А если справа мчит скорая помощь с мигалками?\n","Ответ — хрен знает, никак, магии не бывает, исследователи постоянно этим занимаются, изобретая свои костыли. Одни прописывают все ситуации руками, что позволяет им обрабатывать исключительные случаи типа проблемы вагонетки. Другие идут глубже и отдают эту работу нейросетям, пусть сами всё найдут. Так вместо Q-learning'а у нас появляется Deep Q-Network (DQN).\n","Reinforcement Learning для простого обывателя выглядит как настоящий интеллект. Потому что ух ты, машина сама принимает решения в реальных ситуациях! Он сейчас на хайпе, быстро прёт вперёд и активно пытается в нейросети, чтобы стать еще точнее (а не стукаться о ножку стула по двадцать раз).\n","Потому если вы любите наблюдать результаты своих трудов и хотите популярности — смело прыгайте в методы обучения с подкреплением (до чего ужасный русский термин, каждый раз передёргивает) и заводите канал на ютюбе! Даже я бы смотрел.\n","Помню, у меня в студенчестве были очень популярны генетические алгоритмы (по ссылке прикольная визуализация). Это когда мы бросаем кучу роботов в среду и заставляем их идти к цели, пока не сдохнут. Затем выбираем лучших, скрещиваем, добавляем мутации и бросаем еще раз. Через пару миллиардов лет должно получиться разумное существо. Теория эволюции в действии.\n","Так вот, генетические алгоритмы тоже относятся к обучению с подкреплением, и у них есть важнейшая особенность, подтвержденная многолетней практикой — они нахер никому не нужны.\n","Человечеству еще не удалось придумать задачу, где они были бы реально эффективнее других. Зато отлично заходят как студенческие эксперименты и позволяют кадрить научруков «достижениями» особо не заморачиваясь. На ютюбе тоже зайдёт.\n","Часть 3. Ансамбли\n","«Куча глупых деревьев учится исправлять ошибки друг друга»\n","Сегодня используют для:\n","Всего, где подходят классические алгоритмы (но работают точнее)\n","Поисковые системы (?)\n","Компьютерное зрение\n","Распознавание объектов\n","Популярные алгоритмы: Random Forest, Gradient Boosting\n","Теперь к настоящим взрослым методам. Ансамбли и нейросети — наши главные бойцы на пути к неминуемой сингулярности. Сегодня они дают самые точные результаты и используются всеми крупными компаниями в продакшене. Только о нейросетях трещат на каждом углу, а слова «бустинг» и «бэггинг», наверное, пугают только хипстеров с теккранча.\n","При всей их эффективности, идея до издевательства проста. Оказывается, если взять несколько не очень эффективных методов обучения и обучить исправлять ошибки друг друга, качество такой системы будет аж сильно выше, чем каждого из методов по отдельности.\n","Причём даже лучше, когда взятые алгоритмы максимально нестабильны и сильно плавают от входных данных. Поэтому чаще берут Регрессию и Деревья Решений, которым достаточно одной сильной аномалии в данных, чтобы поехала вся модель. А вот Байеса и K-NN не берут никогда — они хоть и тупые, но очень стабильные.\n","Ансамбль можно собрать как угодно, хоть случайно нарезать в тазик классификаторы и залить регрессией. За точность, правда, тогда никто не ручается. Потому есть три проверенных способа делать ансамбли.\n","Стекинг Обучаем несколько разных алгоритмов и передаём их результаты на вход последнему, который принимает итоговое решение. Типа как девочки сначала опрашивают всех своих подружек, чтобы принять решение встречаться с парнем или нет.\n","Ключевое слово — разных алгоритмов, ведь один и тот же алгоритм, обученный на одних и тех же данных не имеет смысла. Каких — ваше дело, разве что в качестве решающего алгоритма чаще берут регрессию.\n","Чисто из опыта — стекинг на практике применяется редко, потому что два других метода обычно точнее.\n","Беггинг Он же Bootstrap AGGregatING. Обучаем один алгоритм много раз на случайных выборках из исходных данных. В самом конце усредняем ответы.\n","Данные в случайных выборках могут повторяться. То есть из набора 1-2-3 мы можем делать выборки 2-2-3, 1-2-2, 3-1-2 и так пока не надоест. На них мы обучаем один и тот же алгоритм несколько раз, а в конце вычисляем ответ простым голосованием.\n","Самый популярный пример беггинга — алгоритм Random Forest, беггинг на деревьях, который и нарисован на картинке. Когда вы открываете камеру на телефоне и видите как она очертила лица людей в кадре желтыми прямоугольниками — скорее всего это их работа. Нейросеть будет слишком медлительна в реальном времени, а беггинг идеален, ведь он может считать свои деревья параллельно на всех шейдерах видеокарты.\n","Дикая способность параллелиться даёт беггингу преимущество даже над следующим методом, который работает точнее, но только в один поток. Хотя можно разбить на сегменты, запустить несколько... ах кого я учу, сами не маленькие.\n","Бустинг Обучаем алгоритмы последовательно, каждый следующий уделяет особое внимание тем случаям, на которых ошибся предыдущий.\n","Как в беггинге, мы делаем выборки из исходных данных, но теперь не совсем случайно. В каждую новую выборку мы берём часть тех данных, на которых предыдущий алгоритм отработал неправильно. То есть как бы доучиваем новый алгоритм на ошибках предыдущего.\n","Плюсы — неистовая, даже нелегальная в некоторых странах, точность классификации, которой позавидуют все бабушки у подъезда. Минусы уже названы — не параллелится. Хотя всё равно работает быстрее нейросетей, которые как гружёные камазы с песком по сравнению с шустрым бустингом.\n","Нужен реальный пример работы бустинга — откройте Яндекс и введите запрос. Слышите, как Матрикснет грохочет деревьями и ранжирует вам результаты? Вот это как раз оно, Яндекс сейчас весь на бустинге. Про Google не знаю.\n","Сегодня есть три популярных метода бустинга, отличия которых хорошо донесены в статье CatBoost vs. LightGBM vs. XGBoost\n","Часть 4. Нейросети и глубокое обучение\n","«У нас есть сеть из тысячи слоёв, десятки видеокарт, но мы всё еще не придумали где это может быть полезно. Пусть рисует котиков!»\n","Сегодня используют для:\n","Вместо всех вышеперечисленных алгоритмов вообще\n","Определение объектов на фото и видео\n","Распознавание и синтез речи\n","Обработка изображений, перенос стиля\n","Машинный перевод\n","Популярные архитектуры: Перцептрон, Свёрточные Сети (CNN), Рекуррентные Сети (RNN), Автоэнкодеры\n","Если вам хоть раз не пытались объяснить нейросеть на примере якобы работы мозга, расскажите, как вам удалось спрятаться? Я буду избегать этих аналогий и объясню как нравится мне.\n","Любая нейросеть — это набор нейронов и связей между ними. Нейрон лучше всего представлять просто как функцию с кучей входов и одним выходом. Задача нейрона — взять числа со своих входов, выполнить над ними функцию и отдать результат на выход. Простой пример полезного нейрона: просуммировать все цифры со входов, и если их сумма больше N — выдать на выход единицу, иначе — ноль.\n","Связи — это каналы, через которые нейроны шлют друг другу циферки. У каждой связи есть свой вес — её единственный параметр, который можно условно представить как прочность связи. Когда через связь с весом 0.5 проходит число 10, оно превращается в 5. Сам нейрон не разбирается, что к нему пришло и суммирует всё подряд — вот веса и нужны, чтобы управлять на какие входы нейрон должен реагировать, а на какие нет.\n","Чтобы сеть не превратилась в анархию, нейроны решили связывать не как захочется, а по слоям. Внутри одного слоя нейроны никак не связаны, но соединены с нейронами следующего и предыдущего слоя. Данные в такой сети идут строго в одном направлении — от входов первого слоя к выходам последнего.\n","Если нафигачить достаточное количество слоёв и правильно расставить веса в такой сети, получается следующее — подав на вход, скажем, изображение написанной от руки цифры 4, чёрные пиксели активируют связанные с ними нейроны, те активируют следующие слои, и так далее и далее, пока в итоге не загорится самый выход, отвечающий за четвёрку. Результат достигнут.\n","В реальном программировании, естественно, никаких нейронов и связей не пишут, всё представляют матрицами и считают матричными произведениями, потому что нужна скорость. У меня есть два любимых видео, в которых весь описанный мной процесс наглядно объяснён на примере распознавания рукописных цифр. Посмотрите, если хотите разобраться.\n","Такая сеть, где несколько слоёв и между ними связаны все нейроны, называется перцептроном (MLP) и считается самой простой архитектурой для новичков. В боевых задачах лично я никогда её не встречал.\n","Когда мы построили сеть, наша задача правильно расставить веса, чтобы нейроны реагировали на нужные сигналы. Тут нужно вспомнить, что у нас же есть данные — примеры «входов» и правильных «выходов». Будем показывать нейросети рисунок той же цифры 4 и говорить «подстрой свои веса так, чтобы на твоём выходе при таком входе всегда загоралась четвёрка».\n","Сначала все веса просто расставлены случайно, мы показываем сети цифру, она выдаёт какой-то случайный ответ (весов-то нет), а мы сравниваем, насколько результат отличается от нужного нам. Затем идём по сети в обратном направлении, от выходов ко входам, и говорим каждому нейрону — так, ты вот тут зачем-то активировался, из-за тебя всё пошло не так, давай ты будешь чуть меньше реагировать на вот эту связь и чуть больше на вон ту, ок?\n","Через тысяч сто таких циклов «прогнали-проверили-наказали» есть надежда, что веса в сети откорректируются так, как мы хотели. Научно этот подход называется Backpropagation или «Метод обратного распространения ошибки». Забавно то, что чтобы открыть этот метод понадобилось двадцать лет. До него нейросети обучали как могли.\n","Второй мой любимый видос более подробно объясняет весь процесс, но всё так же просто, на пальцах.\n","Хорошо обученная нейросеть могла притворяться любым алгоритмом из этой статьи, а зачастую даже работать точнее. Такая универсальность сделала их дико популярными. Наконец-то у нас есть архитектура человеческого мозга, говорили они, нужно просто собрать много слоёв и обучить их на любых данных, надеялись они. Потом началась первая Зима ИИ, потом оттепель, потом вторая волна разочарования.\n","Оказалось, что на обучение сети с большим количеством слоёв требовались невозможные по тем временам мощности. Сейчас любое игровое ведро с жифорсами превышает мощность тогдашнего датацентра. Тогда даже надежды на это не было, и в нейросетях все сильно разочаровались.\n","Пока лет десять назад не бомбанул диплёрнинг.\n","На английской википедии есть страничка Timeline of machine learning, где хорошо видны всплески радости и волны отчаяния.\n","В 2012 году свёрточная нейросеть порвала всех в конкурсе ImageNet, из-за чего в мире внезапно вспомнили о методах глубокого обучения, описанных еще в 90-х годах. Теперь-то у нас есть видеокарты!\n","Отличие глубокого обучения от классических нейросетей было в новых методах обучения, которые справлялись с большими размерами сетей. Однако сегодня лишь теоретики разделяют, какое обучение можно считать глубоким, а какое не очень. Мы же, как практики, используем популярные «глубокие» библиотеки типа Keras, TensorFlow и PyTorch даже когда нам надо собрать мини-сетку на пять слоёв. Просто потому что они удобнее всего того, что было раньше. Мы называем это просто нейросетями.\n","Расскажу о двух главных на сегодняшний момент.\n","Свёрточные Нейросети (CNN)\n","Свёрточные сети сейчас на пике популярности. Они используются для поиска объектов на фото и видео, распознавания лиц, переноса стиля, генерации и дорисовки изображений, создания эффектов типа слоу-мо и улучшения качества фотографий. Сегодня CNN применяют везде, где есть картинки или видео. Даже в вашем айфоне несколько таких сетей смотрят на ваши голые фотографии, чтобы распознать объекты на них. Если там, конечно, есть что распознавать хехехе.\n","Картинка выше — результат работы библиотеки Detectron, которую Facebook недавно заопенсорсил\n","Проблема с изображениями всегда была в том, что непонятно, как выделять на них признаки. Текст можно разбить по предложениям, взять свойства слов из словарей. Картинки же приходилось размечать руками, объясняя машине, где у котика на фотографии ушки, а где хвост. Такой подход даже назвали «handcrafting признаков» и раньше все так и делали.\n","Проблем у ручного крафтинга много.\n","Во-первых, если котик на фотографии прижал ушки или отвернулся — всё, нейросеть ничего не увидит.\n","Во-вторых, попробуйте сами сейчас назвать хотя бы десять характерных признаков, отличающих котиков от других животных. Я вот не смог. Однако когда ночью мимо меня пробегает чёрное пятно, даже краем глаза я могу сказать котик это или крыса. Потому что человек не смотрит только на форму ушей и количество лап — он оценивает объект по куче разных признаков, о которых сам даже не задумывается. А значит, не понимает и не может объяснить машине.\n","Получается, машине надо самой учиться искать эти признаки, составляя из каких-то базовых линий. Будем делать так: для начала разделим изображение на блоки 8x8 пикселей и выберем какая линия доминирует в каждом — горизонтальная [-], вертикальная [|] или одна из диагональных [/]. Могут и две, и три, так тоже бывает, мы не всегда точно уверены.\n","На выходе мы получим несколько массивов палочек, которые по сути являются простейшими признаками наличия очертаний объектов на картинке. По сути это тоже картинки, просто из палочек. Значит мы можем вновь выбрать блок 8x8 и посмотреть уже, как эти палочки сочетаются друг с другом. А потом еще и еще.\n","Такая операция называется свёрткой, откуда и пошло название метода. Свёртку можно представить как слой нейросети, ведь нейрон — абсолютно любая функция.\n","Когда мы прогоняем через нашу нейросеть кучу фотографий котов, она автоматически расставляет большие веса тем сочетаниям из палочек, которые увидела чаще всего. Причём неважно, это прямая линия спины или сложный геометрический объект типа мордочки — что-то обязательно будет ярко активироваться.\n","На выходе же мы поставим простой перцептрон, который будет смотреть какие сочетания активировались и говорить кому они больше характерны — кошке или собаке.\n","Красота идеи в том, что у нас получилась нейросеть, которая сама находит характерные признаки объектов. Нам больше не надо отбирать их руками. Мы можем сколько угодно кормить её изображениями любых объектов, просто нагуглив миллион картинок с ними — сеть сама составит карты признаков из палочек и научится определять что угодно.\n","По этому поводу у меня даже есть несмешная шутка:\n","Дай нейросети рыбу — она сможет определять рыбу до конца жизни. Дай нейросети удочку — она сможет определять и удочку до конца жизни...\n","Рекуррентные Нейросети (RNN)\n","Вторая по популярности архитектура на сегодняшний день. Благодаря рекуррентным сетям у нас есть такие полезные вещи, как машинный перевод текстов (читайте мой пост об этом) и компьютерный синтез речи. На них решают все задачи, связанные с последовательностями — голосовые, текстовые или музыкальные.\n","Помните олдскульные голосовые синтезаторы типа Microsoft Sam из Windows XP, который смешно произносил слова по буквам, пытаясь как-то склеить их между собой? А теперь посмотрите на Amazon Alexa или Алису от Яндекса — они сегодня не просто произносят слова без ошибок, они даже расставляют акценты в предложении!\n"," Нейросеть учится разговаривать\n","Потому что современные голосовые помощники обучают говорить не буквами, а фразами. Но сразу заставить нейросеть целиком выдавать фразы не выйдет, ведь тогда ей надо будет запомнить все фразы в языке и её размер будет исполинским. Тут на помощь приходит то, что текст, речь или музыка — это последовательности. Каждое слово или звук — как бы самостоятельная единица, но которая зависит от предыдущих. Когда эта связь теряется — получатся дабстеп.\n","Достаточно легко обучить сеть произносить отдельные слова или буквы. Берём кучу размеченных на слова аудиофайлов и обучаем по входному слову выдавать нам последовательность сигналов, похожих на его произношение. Сравниваем с оригиналом от диктора и пытаемся максимально приблизиться к идеалу. Для такого подойдёт даже перцептрон.\n","Вот только с последовательностью опять беда, ведь перцептрон не запоминает что он генерировал ранее. Для него каждый запуск как в первый раз. Появилась идея добавить к каждому нейрону память. Так были придуманы рекуррентные сети, в которых каждый нейрон запоминал все свои предыдущие ответы и при следующем запуске использовал их как дополнительный вход. То есть нейрон мог сказать самому себе в будущем — эй, чувак, следующий звук должен звучать повыше, у нас тут гласная была (очень упрощенный пример).\n","Была лишь одна проблема — когда каждый нейрон запоминал все прошлые результаты, в сети образовалось такое дикое количество входов, что обучить такое количество связей становилось нереально.\n","Когда нейросеть не умеет забывать — её нельзя обучить (у людей та же фигня).\n","Сначала проблему решили в лоб — обрубили каждому нейрону память. Но потом придумали в качестве этой «памяти» использовать специальные ячейки, похожие на память компьютера или регистры процессора. Каждая ячейка позволяла записать в себя циферку, прочитать или сбросить — их назвали ячейки долгой и краткосрочной памяти (LSTM).\n","Когда нейрону было нужно поставить себе напоминалку на будущее — он писал это в ячейку, когда наоборот вся история становилась ненужной (предложение, например, закончилось) — ячейки сбрасывались, оставляя только «долгосрочные» связи, как в классическом перцептроне. Другими словами, сеть обучалась не только устанавливать текущие связи, но и ставить напоминалки.\n","Просто, но работает!\n"," CNN + RNN = фейковый Обама\n","Озвученные тексты для обучения начали брать откуда угодно. Даже базфид смог выгрузить видеозаписи выступлений Обамы и весьма неплохо научить нейросеть разговаривать его голосом. На этом примере видно, что имитировать голос — достаточно простая задача для сегодняшних машин. С видео посложнее, но это пока.\n","Про архитектуры нейросетей можно говорить бесконечно. Любознательных отправляю смотреть схему и читать статью Neural Network Zoo, где собраны все типы нейронных сетей. Есть и русская версия.\n","Заключение: когда на войну с машинами?\n","На вопрос «когда машины станут умнее нас и всех поработят?» я всегда отвечаю, что он заранее неправильный. В нём слишком много скрытых условий, который примаются как как данность.\n","Вот мы говорим «станут умнее нас». Значит мы подразумеваем, что существует некая единая шкала интеллекта, наверху которой находится человек, собаки пониже, а глупые голуби тусят в самом низу. Получается человек должен превосходить нижестоящих животных во всём, так? А в жизни не так. Средняя белка может помнить тысячу тайников в орешками, а я не могу вспомнить где ключи. Получается интеллект — это набор разных навыков, а не единая измеримая величина? Или просто запоминание орешков в него не входит? А убивание человеков входит?\n","Ну и самый интересный для меня вопрос — почему мы заранее считаем, что возможности человеческого мозга ограничены? В интернетах обожают рисовать графики, на которых технологический прогресс обозначен экспонентой, а возможности кожаных мешков константой. Но так ли это?\n","Вот давайте, прямо сейчас в уме умножьте 1680 на 950. Да, знаю, вы даже пытаться не станете. Но дай вам калькулятор, это займёт две секунды. Значит ли это, что вы только что расширили возможности своего мозга с помощью калькулятора? Можно ли продолжать их расширять другими машинами? Я вот использую заметки на телефоне — значит ли это, что я расширяю свою память с помощью машины?\n","Получается, мы уже успешно расширяем способности нашего мозга с помощью машин. Или нет?\n","Подумайте. У меня всё.\n","'''"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzA8Z1ylnWFl","executionInfo":{"status":"ok","timestamp":1619379214404,"user_tz":-180,"elapsed":1444,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["raw = raw.lower() "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Aa2mJfLnhMC","executionInfo":{"status":"ok","timestamp":1619379222235,"user_tz":-180,"elapsed":3182,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"4e6d422d-996f-4397-be00-63557045dcdc"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"iYuMLvt0nmOW","executionInfo":{"status":"ok","timestamp":1619379226761,"user_tz":-180,"elapsed":1561,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["sent_tokens = nltk.sent_tokenize(raw)\n","word_tokens = nltk.word_tokenize(raw)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"OFwqRC95O4ws","executionInfo":{"status":"ok","timestamp":1619379231608,"user_tz":-180,"elapsed":1793,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"f49faebb-240b-4d70-9933-2bb2f9881df8"},"source":["raw"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nмашинное обучение для людей\\nразбираемся простыми словами\\nмашинное обучение — как секс в старших классах. все говорят о нем по углам, единицы понимают, а занимается только препод. статьи о машинном обучении делятся на два типа: это либо трёхтомники с формулами и теоремами, которые я ни разу не смог дочитать даже до середины, либо сказки об искусственном интеллекте, профессиях будущего и волшебных дата-саентистах.\\nзачем обучать машины\\nснова разберём на олегах.\\nпредположим, олег хочет купить автомобиль и считает сколько денег ему нужно для этого накопить. он пересмотрел десяток объявлений в интернете и увидел, что новые автомобили стоят около $20 000, годовалые — примерно $19 000, двухлетние — $18 000 и так далее.\\nв уме олег-аналитик выводит формулу: адекватная цена автомобиля начинается от $20 000 и падает на $1000 каждый год, пока не упрётся в $10 000.\\nолег сделал то, что в машинном обучении называют регрессией — предсказал цену по известным данным. люди делают это постоянно, когда считают почём продать старый айфон или сколько шашлыка взять на дачу (моя формула — полкило на человека в сутки).\\nда, было бы удобно иметь формулу под каждую проблему на свете. но взять те же цены на автомобили: кроме пробега есть десятки комплектаций, разное техническое состояние, сезонность спроса и еще столько неочевидных факторов, которые олег, даже при всём желании, не учел бы в голове.\\nлюди тупы и ленивы — надо заставить вкалывать роботов. пусть машина посмотрит на наши данные, найдёт в них закономерности и научится предсказывать для нас ответ. самое интересное, что в итоге она стала находить даже такие закономерности, о которых люди не догадывались.\\nтак родилось машинное обучение.\\nтри составляющие обучения\\nцель машинного обучения — предсказать результат по входным данным. чем разнообразнее входные данные, тем проще машине найти закономерности и тем точнее результат.\\nитак, если мы хотим обучить машину, нам нужны три вещи:\\nданные хотим определять спам — нужны примеры спам-писем, предсказывать курс акций — нужна история цен, узнать интересы пользователя — нужны его лайки или посты. данных нужно как можно больше. десятки тысяч примеров — это самый злой минимум для отчаянных.\\nданные собирают как могут. кто-то вручную — получается дольше, меньше, зато без ошибок. кто-то автоматически — просто сливает машине всё, что нашлось, и верит в лучшее. самые хитрые, типа гугла, используют своих же пользователей для бесплатной разметки. вспомните recaptcha, которая иногда требует «найти на фотографии все дорожные знаки» — это оно и есть.\\nза хорошими наборами данных (датасетами) идёт большая охота. крупные компании, бывает, раскрывают свои алгоритмы, но датасеты — крайне редко.\\nпризнаки мы называем их фичами (features), так что ненавистникам англицизмов придётся страдать. фичи, свойства, характеристики, признаки — ими могут быть пробег автомобиля, пол пользователя, цена акций, даже счетчик частоты появления слова в тексте может быть фичей.\\nмашина должна знать, на что ей конкретно смотреть. хорошо, когда данные просто лежат в табличках — названия их колонок и есть фичи. а если у нас сто гигабайт картинок с котами? когда признаков много, модель работает медленно и неэффективно. зачастую отбор правильных фич занимает больше времени, чем всё остальное обучение. но бывают и обратные ситуации, когда кожаный мешок сам решает отобрать только «правильные» на его взгляд признаки и вносит в модель субъективность — она начинает дико врать.\\nалгоритм одну задачу можно решить разными методами примерно всегда. от выбора метода зависит точность, скорость работы и размер готовой модели. но есть один нюанс: если данные говно, даже самый лучший алгоритм не поможет. не зацикливайтесь на процентах, лучше соберите побольше данных.\\nобучение vs интеллект\\nоднажды в одном хипстерском издании я видел статью под заголовком «заменят ли нейросети машинное обучение». пиарщики в своих пресс-релизах обзывают «искусственным интеллектом» любую линейную регрессию, с которой уже дети во дворе играют. объясняю разницу на картинке, раз и навсегда.\\nискусственный интеллект — название всей области, как биология или химия.\\nмашинное обучение — это раздел искусственного интеллекта. важный, но не единственный.\\nнейросети — один из видов машинного обучения. популярный, но есть и другие, не хуже.\\nглубокое обучение — архитектура нейросетей, один из подходов к их построению и обучению. на практике сегодня мало кто отличает, где глубокие нейросети, а где не очень. говорят название конкретной сети и всё.\\nсравнивать можно только вещи одного уровня, иначе получается полный буллщит типа «что лучше: машина или колесо?» не отождествляйте термины без причины, чтобы не выглядеть дурачком.\\nвот что машины сегодня умеют, а что не под силу даже самым обученным.\\nмашина может: предсказывать запоминать воспроизводить выбирать лучшее\\nмашина не может: создавать новое резко поумнеть\\tвыйти за рамки задачи убить всех людей\\nдумаю потом нарисовать полноценную настенную карту со стрелочками и объяснениями, что где используется, если статья зайдёт.\\nи да. классифицировать алгоритмы можно десятком способов. я выбрал этот, потому что он мне кажется самым удобным для повествования. надо понимать, что не бывает так, чтобы задачу решал только один метод. я буду упоминать известные примеры применений, но держите в уме, что «сын маминой подруги» всё это может решить нейросетями.\\nначну с базового обзора. сегодня в машинном обучении есть всего четыре основных направления.\\nчасть 1. классическое обучение\\nпервые алгоритмы пришли к нам из чистой статистики еще в 1950-х. они решали формальные задачи — искали закономерности в циферках, оценивали близость точек в пространстве и вычисляли направления.\\nсегодня на классических алгоритмах держится добрая половина интернета. когда вы встречаете блок «рекомендованные статьи» на сайте, или банк блокирует все ваши деньги на карточке после первой же покупки кофе за границей — это почти всегда дело рук одного из этих алгоритмов.\\nда, крупные корпорации любят решать все проблемы нейросетями. потому что лишние 2% точности для них легко конвертируются в дополнительные 2 миллиарда прибыли. остальным же стоит включать голову. когда задача решаема классическими методами, дешевле реализовать сколько-нибудь полезную для бизнеса систему на них, а потом думать об улучшениях. а если вы не решили задачу, то не решить её на 2% лучше вам не особо поможет.\\nзнаю несколько смешных историй, когда команда три месяца переписывала систему рекомендаций интернет-магазина на более точный алгоритм, и только потом понимала, что покупатели вообще ей не пользуются. большая часть просто приходит из поисковиков.\\nпри всей своей популярности, классические алгоритмы настолько просты, что их легко объяснить даже ребёнку. сегодня они как основы арифметики — пригождаются постоянно, но некоторые всё равно стали их забывать.\\nобучение с учителем\\nклассическое обучение любят делить на две категории — с учителем и без. часто можно встретить их английские наименования — supervised и unsupervised learning.\\nв первом случае у машины есть некий учитель, который говорит ей как правильно. рассказывает, что на этой картинке кошка, а на этой собака. то есть учитель уже заранее разделил (разметил) все данные на кошек и собак, а машина учится на конкретных примерах.\\nв обучении без учителя, машине просто вываливают кучу фотографий животных на стол и говорят «разберись, кто здесь на кого похож». данные не размечены, у машины нет учителя, и она пытается сама найти любые закономерности. об этих методах поговорим ниже.\\nочевидно, что с учителем машина обучится быстрее и точнее, потому в боевых задачах его используют намного чаще. эти задачи делятся на два типа: классификация — предсказание категории объекта, и регрессия — предсказание места на числовой прямой.\\nклассификация\\n«разделяет объекты по заранее известному признаку. носки по цветам, документы по языкам, музыку по жанрам»\\nсегодня используют для:\\nспам-фильтры\\nопределение языка\\nпоиск похожих документов\\nанализ тональности\\nраспознавание рукописных букв и цифр\\nопределение подозрительных транзакций\\nпопулярные алгоритмы: наивный байес, деревья решений, логистическая регрессия, k-ближайших соседей, машины опорных векторов\\nздесь и далее в комментах можно дополнять эти блоки. приводите свои примеры задач, областей и алгоритмов, потому что описанные мной взяты из субъективного опыта.\\nклассификация вещей — самая популярная задача во всём машинном обучении. машина в ней как ребёнок, который учится раскладывать игрушки: роботов в один ящик, танки в другой. опа, а если это робот-танк? штош, время расплакаться и выпасть в ошибку.\\n старый доклад бобука про повышение конверсии лендингов с помощью svm\\nдля классификации всегда нужен учитель — размеченные данные с признаками и категориями, которые машина будет учиться определять по этим признакам. дальше классифицировать можно что угодно: пользователей по интересам — так делают алгоритмические ленты, статьи по языкам и тематикам — важно для поисковиков, музыку по жанрам — вспомните плейлисты спотифая и яндекс.музыки, даже письма в вашем почтовом ящике.\\nраньше все спам-фильтры работали на алгоритме наивного байеса. машина считала сколько раз слово «виагра» встречается в спаме, а сколько раз в нормальных письмах. перемножала эти две вероятности по формуле байеса, складывала результаты всех слов и бац, всем лежать, у нас машинное обучение!\\nпозже спамеры научились обходить фильтр байеса, просто вставляя в конец письма много слов с «хорошими» рейтингами. метод получил ироничное название отравление байеса, а фильтровать спам стали другими алгоритмами. но метод навсегда остался в учебниках как самый простой, красивый и один из первых практически полезных.\\nвозьмем другой пример полезной классификации. вот берёте вы кредит в банке. как банку удостовериться, вернёте вы его или нет? точно никак, но у банка есть тысячи профилей других людей, которые уже брали кредит до вас. там указан их возраст, образование, должность, уровень зарплаты и главное — кто из них вернул кредит, а с кем возникли проблемы.\\nда, все догадались, где здесь данные и какой надо предсказать результат. обучим машину, найдём закономерности, получим ответ — вопрос не в этом. проблема в том, что банк не может слепо доверять ответу машины, без объяснений. вдруг сбой, злые хакеры или бухой админ решил скриптик исправить.\\nдля этой задачи придумали деревья решений. машина автоматически разделяет все данные по вопросам, ответы на которые «да» или «нет». вопросы могут быть не совсем адекватными с точки зрения человека, например «зарплата заёмщика больше, чем 25934 рубля?», но машина придумывает их так, чтобы на каждом шаге разбиение было самым точным.\\nтак получается дерево вопросов. чем выше уровень, тем более общий вопрос. потом даже можно загнать их аналитикам, и они навыдумывают почему так.\\nдеревья нашли свою нишу в областях с высокой ответственностью: диагностике, медицине, финансах.\\nдва самых популярных алгоритма построения деревьев — cart и c4.5.\\nв чистом виде деревья сегодня используют редко, но вот их ансамбли (о которых будет ниже) лежат в основе крупных систем и зачастую уделывают даже нейросети. например, когда вы задаете вопрос яндексу, именно толпа глупых деревьев бежит ранжировать вам результаты.\\nно самым популярным методом классической классификации заслуженно является метод опорных векторов (svm). им классифицировали уже всё: виды растений, лица на фотографиях, документы по тематикам, даже странных playboy-моделей. много лет он был главным ответом на вопрос «какой бы мне взять классификатор».\\nидея svm по своей сути проста — он ищет, как так провести две прямые между категориями, чтобы между ними образовался наибольший зазор. на картинке видно нагляднее:\\nу классификации есть полезная обратная сторона — поиск аномалий. когда какой-то признак объекта сильно не вписывается в наши классы, мы ярко подсвечиваем его на экране. сейчас так делают в медицине: компьютер подсвечивает врачу все подозрительные области мрт или выделяет отклонения в анализах. на биржах таким же образом определяют нестандартных игроков, которые скорее всего являются инсайдерами. научив компьютер «как правильно», мы автоматически получаем и обратный классификатор — как неправильно.\\nсегодня для классификации всё чаще используют нейросети, ведь по сути их для этого и изобрели.\\nправило буравчика такое: сложнее данные — сложнее алгоритм. для текста, цифр, табличек я бы начинал с классики. там модели меньше, обучаются быстрее и работают понятнее. для картинок, видео и другой непонятной бигдаты — сразу смотрел бы в сторону нейросетей.\\nлет пять назад еще можно было встретить классификатор лиц на svm, но сегодня под эту задачу сотня готовых сеток по интернету валяются, чо бы их не взять. а вот спам-фильтры как на svm писали, так и не вижу смысла останавливаться.\\nрегрессия\\n«нарисуй линию вдоль моих точек. да, это машинное обучение»\\nсегодня используют для:\\nпрогноз стоимости ценных бумаг\\nанализ спроса, объема продаж\\nмедицинские диагнозы\\nлюбые зависимости числа от времени\\nпопулярные алгоритмы: линейная или полиномиальная регрессия\\nрегрессия — та же классификация, только вместо категории мы предсказываем число. стоимость автомобиля по его пробегу, количество пробок по времени суток, объем спроса на товар от роста компании и.т.д. на регрессию идеально ложатся любые задачи, где есть зависимость от времени.\\nрегрессию очень любят финансисты и аналитики, она встроена даже в excel. внутри всё работает, опять же, банально: машина тупо пытается нарисовать линию, которая в среднем отражает зависимость. правда, в отличии от человека с фломастером и вайтбордом, делает она это математически точно — считая среднее расстояние до каждой точки и пытаясь всем угодить.\\nкогда регрессия рисует прямую линию, её называют линейной, когда кривую — полиномиальной. это два основных вида регрессии, дальше уже начинаются редкоземельные методы. но так как в семье не без урода, есть логистическая регрессия, которая на самом деле не регрессия, а метод классификации, от чего у всех постоянно путаница. не делайте так.\\nсхожесть регрессии и классификации подтверждается еще и тем, что многие классификаторы, после небольшого тюнинга, превращаются в регрессоры. например, мы можем не просто смотреть к какому классу принадлежит объект, а запоминать, насколько он близок — и вот, у нас регрессия.\\nдля желающих понять это глубже, но тоже простыми словами, рекомендую цикл статей machine learning for humans\\nобучение без учителя\\nобучение без учителя (unsupervised learning) было изобретено позже, аж в 90-е, и на практике используется реже. но бывают задачи, где у нас просто нет выбора.\\nразмеченные данные, как я сказал, дорогая редкость. но что делать если я хочу, например, написать классификатор автобусов — идти на улицу руками фотографировать миллион сраных икарусов и подписывать где какой? так и жизнь вся пройдёт, а у меня еще игры в стиме не пройдены.\\nкогда нет разметки, есть надежда на капитализм, социальное расслоение и миллион китайцев из сервисов типа яндекс.толока, которые готовы делать для вас что угодно за пять центов. так обычно и поступают на практике. а вы думали где яндекс берёт все свои крутые датасеты?\\nлибо, можно попробовать обучение без учителя. хотя, честно говоря, из своей практики я не помню чтобы где-то оно сработало хорошо.\\nобучение без учителя, всё же, чаще используют как метод анализа данных, а не как основной алгоритм. специальный кожаный мешок с дипломом мгу вбрасывает туда кучу мусора и наблюдает. кластеры есть? зависимости появились? нет? ну штош, продолжай, труд освобождает. тыж хотел работать в датасаенсе.\\nкластеризация\\n«разделяет объекты по неизвестному признаку. машина сама решает как лучше»\\nсегодня используют для:\\nсегментация рынка (типов покупателей, лояльности)\\nобъединение близких точек на карте\\nсжатие изображений\\nанализ и разметки новых данных\\nдетекторы аномального поведения\\nпопулярные алгоритмы: метод k-средних, mean-shift, dbscan\\n 2 комментария\\nкластеризация — это классификация, но без заранее известных классов. она сама ищет похожие объекты и объединяет их в кластеры. количество кластеров можно задать заранее или доверить это машине. похожесть объектов машина определяет по тем признакам, которые мы ей разметили — у кого много схожих характеристик, тех давай в один класс.\\nотличный пример кластеризации — маркеры на картах в вебе. когда вы ищете все крафтовые бары в москве, движку приходится группировать их в кружочки с циферкой, иначе браузер зависнет в потугах нарисовать миллион маркеров.\\nболее сложные примеры кластеризации можно вспомнить в приложениях iphoto или google photos, которые находят лица людей на фотографиях и группируют их в альбомы. приложение не знает как зовут ваших друзей, но может отличить их по характерным чертам лица. типичная кластеризация.\\nправда для начала им приходится найти эти самые «характерные черты», а это уже только с учителем.\\nсжатие изображений — еще одна популярная проблема. сохраняя картинку в png, вы можете установить палитру, скажем, в 32 цвета. тогда кластеризация найдёт все «примерно красные» пиксели изображения, высчитает из них «средний красный по больнице» и заменит все красные на него. меньше цветов — меньше файл.\\nпроблема только, как быть с цветами типа cyan ?? — вот он ближе к зеленому или синему? тут нам поможет популярный алгоритм кластеризации — метод к-средних (k-means). мы случайным образом бросаем на палитру цветов наши 32 точки, обзывая их центроидами. все остальные точки относим к ближайшему центроиду от них — получаются как бы созвездия из самых близких цветов. затем двигаем центроид в центр своего созвездия и повторяем пока центроиды не перестанут двигаться. кластеры обнаружены, стабильны и их ровно 32 как и надо было.\\nискать центроиды удобно и просто, но в реальных задачах кластеры могут быть совсем не круглой формы. вот вы геолог, которому нужно найти на карте схожие по структуре горные породы — ваши кластеры не только будут вложены друг в друга, но вы ещё и не знаете сколько их вообще получится.\\nхитрым задачам — хитрые методы. dbscan, например. он сам находит скопления точек и строит вокруг кластеры. его легко понять, если представить, что точки — это люди на площади. находим трёх любых близко стоящих человека и говорим им взяться за руки. затем они начинают брать за руку тех, до кого могут дотянуться. так по цепочке, пока никто больше не сможет взять кого-то за руку — это и будет первый кластер. повторяем, пока не поделим всех. те, кому вообще некого брать за руку — это выбросы, аномалии. в динамике выглядит довольно красиво:\\nинтересующимся кластеризацией рекомендую статью the 5 clustering algorithms data scientists need to know\\nкак и классификация, кластеризация тоже может использоваться как детектор аномалий. поведение пользователя после регистрации резко отличается от нормального? заблокировать его и создать тикет саппорту, чтобы проверили бот это или нет. при этом нам даже не надо знать, что есть «нормальное поведение» — мы просто выгружаем все действия пользователей в модель, и пусть машина сама разбирается кто тут нормальный.\\nработает такой подход, по сравнению с классификацией, не очень. но за спрос не бьют, вдруг получится.\\nуменьшение размерности (обобщение)\\n«собирает конкретные признаки в абстракции более высокого уровня»\\nсегодня используют для:\\nрекомендательные системы (?)\\nкрасивые визуализации\\nопределение тематики и поиска похожих документов\\nанализ фейковых изображений\\nриск-менеджмент\\nпопулярные алгоритмы: метод главных компонент (pca), сингулярное разложение (svd), латентное размещение дирихле (lda), латентно-семантический анализ (lsa, plsa, glsa), t-sne (для визуализации)\\nизначально это были методы хардкорных data scientist'ов, которым сгружали две фуры цифр и говорили найти там что-нибудь интересное. когда просто строить графики в экселе уже не помогало, они придумали напрячь машины искать закономерности вместо них. так у них появились методы, которые назвали dimension reduction или feature learning.\\nпроецируем 2d-данные на прямую (pca)\\nдля нас практическая польза их методов в том, что мы можем объединить несколько признаков в один и получить абстракцию. например, собаки с треугольными ушами, длинными носами и большими хвостами соединяются в полезную абстракцию «овчарки». да, мы теряем информацию о конкретных овчарках, но новая абстракция всяко полезнее этих лишних деталей. плюс, обучение на меньшем количестве размерностей идёт сильно быстрее.\\nинструмент на удивление хорошо подошел для определения тематик текстов (topic modelling). мы смогли абстрагироваться от конкретных слов до уровня смыслов даже без привлечения учителя со списком категорий. алгоритм назвали латентно-семантический анализ (lsa), и его идея была в том, что частота появления слова в тексте зависит от его тематики: в научных статьях больше технических терминов, в новостях о политике — имён политиков. да, мы могли бы просто взять все слова из статей и кластеризовать, как мы делали с ларьками выше, но тогда мы бы потеряли все полезные связи между словами, например, что батарейка и аккумулятор, означают одно и то же в разных документах.\\nточность такой системы — полное дно, даже не пытайтесь.\\nнужно как-то объединить слова и документы в один признак, чтобы не терять эти скрытые (латентные) связи. отсюда и появилось название метода. оказалось, что сингулярное разложение (svd) легко справляется с этой задачей, выявляя для нас полезные тематические кластеры из слов, которые встречаются вместе.\\nдля понимания рекомендую статью как уменьшить количество измерений и извлечь из этого пользу, а практическое применение хорошо описано в статье алгоритм lsa для поиска похожих документов.\\nдругое мега-популярное применение метода уменьшения размерности нашли в рекомендательных системах и коллаборативной фильтрации (у меня был пост про их виды). оказалось, если абстрагировать ими оценки пользователей фильмам, получается неплохая система рекомендаций кино, музыки, игр и чего угодно вообще.\\nполученная абстракция будет с трудом понимаема мозгом, но когда исследователи начали пристально рассматривать новые признаки, они обнаружили, что какие-то из них явно коррелируют с возрастом пользователя (дети чаще играли в майнкрафт и смотрели мультфильмы), другие с определёнными жанрами кино, а третьи вообще с синдромом поиска глубокого смысла.\\nмашина, не знавшая ничего кроме оценок пользователей, смогла добраться до таких высоких материй, даже не понимая их. достойно. дальше можно проводить соцопросы и писать дипломные работы о том, почему бородатые мужики любят дегенеративные мультики.\\nна эту тему есть неплохая лекция яндекса — как работают рекомендательные системы\\nпоиск правил (ассоциация)\\n«ищет закономерности в потоке заказов»\\nсегодня используют для:\\nпрогноз акций и распродаж\\nанализ товаров, покупаемых вместе\\nрасстановка товаров на полках\\nанализ паттернов поведения на веб-сайтах\\nпопулярные алгоритмы: apriori, euclat, fp-growth\\nсюда входят все методы анализа продуктовых корзин, стратегий маркетинга и других последовательностей.\\nпредположим, покупатель берёт в дальнем углу магазина пиво и идёт на кассу. стоит ли ставить на его пути орешки? часто ли люди берут их вместе? орешки с пивом, наверное да, но какие ещё товары покупают вместе? когда вы владелец сети гипермаркетов, ответ для вас не всегда очевиден, но одно тактическое улучшение в расстановке товаров может принести хорошую прибыль.\\nто же касается интернет-магазинов, где задача еще интереснее — за каким товаром покупатель вернётся в следующий раз?\\nпо непонятным мне причинам, поиск правил — самая хреново продуманная категория среди всех методов обучения. классические способы заключаются в тупом переборе пар всех купленных товаров с помощью деревьев или множеств. сами алгоритмы работают наполовину — могут искать закономерности, но не умеют обобщать или воспроизводить их на новых примерах.\\nв реальности каждый крупный ритейлер пилит свой велосипед, и никаких особых прорывов в этой области я не встречал. максимальный уровень технологий здесь — запилить систему рекомендаций, как в пункте выше. хотя может я просто далёк от этой области, расскажите в комментах, кто шарит?\\nчасть 2. обучение с подкреплением\\n«брось робота в лабиринт и пусть ищет выход»\\nсегодня используют для:\\nсамоуправляемых автомобилей\\nроботов пылесосов\\nигр\\nавтоматической торговли\\nуправления ресурсами предприятий\\nпопулярные алгоритмы: q-learning, sarsa, dqn, a3c, генетический алгоритм\\nнаконец мы дошли до вещей, которые, вроде, выглядят как настоящий искусственный интеллект. многие авторы почему-то ставят обучение с подкреплением где-то между обучением с учителем и без, но я не понимаю чем они похожи. названием?\\nобучение с подкреплением используют там, где задачей стоит не анализ данных, а выживание в реальной среде.\\n нейросеть играет в марио\\nсредой может быть даже видеоигра. роботы, играющие в марио, были популярны еще лет пять назад. средой может быть реальный мир. как пример — автопилот теслы, который учится не сбивать пешеходов, или роботы-пылесосы, главная задача которых — напугать вашего кота до усрачки с максимальной эффективностью.\\nзнания об окружающем мире такому роботу могут быть полезны, но чисто для справки. не важно сколько данных он соберёт, у него всё равно не получится предусмотреть все ситуации. потому его цель — минимизировать ошибки, а не рассчитать все ходы. робот учится выживать в пространстве с максимальной выгодой: собранными монетками в марио, временем поездки в тесле или количеством убитых кожаных мешков хихихих.\\nвыживание в среде и есть идея обучения с подкреплением. давайте бросим бедного робота в реальную жизнь, будем штрафовать его за ошибки и награждать за правильные поступки. на людях норм работает, почему бы на и роботах не попробовать.\\nумные модели роботов-пылесосов и самоуправляемые автомобили обучаются именно так: им создают виртуальный город (часто на основе карт настоящих городов), населяют случайными пешеходами и отправляют учиться никого там не убивать. когда робот начинает хорошо себя чувствовать в искусственном gta, его выпускают тестировать на реальные улицы.\\nзапоминать сам город машине не нужно — такой подход называется model-free. конечно, тут есть и классический model-based, но в нём нашей машине пришлось бы запоминать модель всей планеты, всех возможных ситуаций на всех перекрёстках мира. такое просто не работает. в обучении с подкреплением машина не запоминает каждое движение, а пытается обобщить ситуации, чтобы выходить из них с максимальной выгодой.\\nпомните новость пару лет назад, когда машина обыграла человека в го? хотя незадолго до этого было доказано, что число комбинаций физически невозможно просчитать, ведь оно превышает количество атомов во вселенной. то есть если в шахматах машина реально просчитывала все будущие комбинации и побеждала, с го так не прокатывало. поэтому она просто выбирала наилучший выход из каждой ситуации и делала это достаточно точно, чтобы обыграть кожаного ублюдка.\\nэта идея лежит в основе алгоритма q-learning и его производных (sarsa и dqn). буква q в названии означает слово quality, то есть робот учится поступать наиболее качественно в любой ситуации, а все ситуации он запоминает как простой марковский процесс.\\nмашина прогоняет миллионы симуляций в среде, запоминая все сложившиеся ситуации и выходы из них, которые принесли максимальное вознаграждение. но как понять, когда у нас сложилась известная ситуация, а когда абсолютно новая? вот самоуправляемый автомобиль стоит у перекрестка и загорается зелёный — значит можно ехать? а если справа мчит скорая помощь с мигалками?\\nответ — хрен знает, никак, магии не бывает, исследователи постоянно этим занимаются, изобретая свои костыли. одни прописывают все ситуации руками, что позволяет им обрабатывать исключительные случаи типа проблемы вагонетки. другие идут глубже и отдают эту работу нейросетям, пусть сами всё найдут. так вместо q-learning'а у нас появляется deep q-network (dqn).\\nreinforcement learning для простого обывателя выглядит как настоящий интеллект. потому что ух ты, машина сама принимает решения в реальных ситуациях! он сейчас на хайпе, быстро прёт вперёд и активно пытается в нейросети, чтобы стать еще точнее (а не стукаться о ножку стула по двадцать раз).\\nпотому если вы любите наблюдать результаты своих трудов и хотите популярности — смело прыгайте в методы обучения с подкреплением (до чего ужасный русский термин, каждый раз передёргивает) и заводите канал на ютюбе! даже я бы смотрел.\\nпомню, у меня в студенчестве были очень популярны генетические алгоритмы (по ссылке прикольная визуализация). это когда мы бросаем кучу роботов в среду и заставляем их идти к цели, пока не сдохнут. затем выбираем лучших, скрещиваем, добавляем мутации и бросаем еще раз. через пару миллиардов лет должно получиться разумное существо. теория эволюции в действии.\\nтак вот, генетические алгоритмы тоже относятся к обучению с подкреплением, и у них есть важнейшая особенность, подтвержденная многолетней практикой — они нахер никому не нужны.\\nчеловечеству еще не удалось придумать задачу, где они были бы реально эффективнее других. зато отлично заходят как студенческие эксперименты и позволяют кадрить научруков «достижениями» особо не заморачиваясь. на ютюбе тоже зайдёт.\\nчасть 3. ансамбли\\n«куча глупых деревьев учится исправлять ошибки друг друга»\\nсегодня используют для:\\nвсего, где подходят классические алгоритмы (но работают точнее)\\nпоисковые системы (?)\\nкомпьютерное зрение\\nраспознавание объектов\\nпопулярные алгоритмы: random forest, gradient boosting\\nтеперь к настоящим взрослым методам. ансамбли и нейросети — наши главные бойцы на пути к неминуемой сингулярности. сегодня они дают самые точные результаты и используются всеми крупными компаниями в продакшене. только о нейросетях трещат на каждом углу, а слова «бустинг» и «бэггинг», наверное, пугают только хипстеров с теккранча.\\nпри всей их эффективности, идея до издевательства проста. оказывается, если взять несколько не очень эффективных методов обучения и обучить исправлять ошибки друг друга, качество такой системы будет аж сильно выше, чем каждого из методов по отдельности.\\nпричём даже лучше, когда взятые алгоритмы максимально нестабильны и сильно плавают от входных данных. поэтому чаще берут регрессию и деревья решений, которым достаточно одной сильной аномалии в данных, чтобы поехала вся модель. а вот байеса и k-nn не берут никогда — они хоть и тупые, но очень стабильные.\\nансамбль можно собрать как угодно, хоть случайно нарезать в тазик классификаторы и залить регрессией. за точность, правда, тогда никто не ручается. потому есть три проверенных способа делать ансамбли.\\nстекинг обучаем несколько разных алгоритмов и передаём их результаты на вход последнему, который принимает итоговое решение. типа как девочки сначала опрашивают всех своих подружек, чтобы принять решение встречаться с парнем или нет.\\nключевое слово — разных алгоритмов, ведь один и тот же алгоритм, обученный на одних и тех же данных не имеет смысла. каких — ваше дело, разве что в качестве решающего алгоритма чаще берут регрессию.\\nчисто из опыта — стекинг на практике применяется редко, потому что два других метода обычно точнее.\\nбеггинг он же bootstrap aggregating. обучаем один алгоритм много раз на случайных выборках из исходных данных. в самом конце усредняем ответы.\\nданные в случайных выборках могут повторяться. то есть из набора 1-2-3 мы можем делать выборки 2-2-3, 1-2-2, 3-1-2 и так пока не надоест. на них мы обучаем один и тот же алгоритм несколько раз, а в конце вычисляем ответ простым голосованием.\\nсамый популярный пример беггинга — алгоритм random forest, беггинг на деревьях, который и нарисован на картинке. когда вы открываете камеру на телефоне и видите как она очертила лица людей в кадре желтыми прямоугольниками — скорее всего это их работа. нейросеть будет слишком медлительна в реальном времени, а беггинг идеален, ведь он может считать свои деревья параллельно на всех шейдерах видеокарты.\\nдикая способность параллелиться даёт беггингу преимущество даже над следующим методом, который работает точнее, но только в один поток. хотя можно разбить на сегменты, запустить несколько... ах кого я учу, сами не маленькие.\\nбустинг обучаем алгоритмы последовательно, каждый следующий уделяет особое внимание тем случаям, на которых ошибся предыдущий.\\nкак в беггинге, мы делаем выборки из исходных данных, но теперь не совсем случайно. в каждую новую выборку мы берём часть тех данных, на которых предыдущий алгоритм отработал неправильно. то есть как бы доучиваем новый алгоритм на ошибках предыдущего.\\nплюсы — неистовая, даже нелегальная в некоторых странах, точность классификации, которой позавидуют все бабушки у подъезда. минусы уже названы — не параллелится. хотя всё равно работает быстрее нейросетей, которые как гружёные камазы с песком по сравнению с шустрым бустингом.\\nнужен реальный пример работы бустинга — откройте яндекс и введите запрос. слышите, как матрикснет грохочет деревьями и ранжирует вам результаты? вот это как раз оно, яндекс сейчас весь на бустинге. про google не знаю.\\nсегодня есть три популярных метода бустинга, отличия которых хорошо донесены в статье catboost vs. lightgbm vs. xgboost\\nчасть 4. нейросети и глубокое обучение\\n«у нас есть сеть из тысячи слоёв, десятки видеокарт, но мы всё еще не придумали где это может быть полезно. пусть рисует котиков!»\\nсегодня используют для:\\nвместо всех вышеперечисленных алгоритмов вообще\\nопределение объектов на фото и видео\\nраспознавание и синтез речи\\nобработка изображений, перенос стиля\\nмашинный перевод\\nпопулярные архитектуры: перцептрон, свёрточные сети (cnn), рекуррентные сети (rnn), автоэнкодеры\\nесли вам хоть раз не пытались объяснить нейросеть на примере якобы работы мозга, расскажите, как вам удалось спрятаться? я буду избегать этих аналогий и объясню как нравится мне.\\nлюбая нейросеть — это набор нейронов и связей между ними. нейрон лучше всего представлять просто как функцию с кучей входов и одним выходом. задача нейрона — взять числа со своих входов, выполнить над ними функцию и отдать результат на выход. простой пример полезного нейрона: просуммировать все цифры со входов, и если их сумма больше n — выдать на выход единицу, иначе — ноль.\\nсвязи — это каналы, через которые нейроны шлют друг другу циферки. у каждой связи есть свой вес — её единственный параметр, который можно условно представить как прочность связи. когда через связь с весом 0.5 проходит число 10, оно превращается в 5. сам нейрон не разбирается, что к нему пришло и суммирует всё подряд — вот веса и нужны, чтобы управлять на какие входы нейрон должен реагировать, а на какие нет.\\nчтобы сеть не превратилась в анархию, нейроны решили связывать не как захочется, а по слоям. внутри одного слоя нейроны никак не связаны, но соединены с нейронами следующего и предыдущего слоя. данные в такой сети идут строго в одном направлении — от входов первого слоя к выходам последнего.\\nесли нафигачить достаточное количество слоёв и правильно расставить веса в такой сети, получается следующее — подав на вход, скажем, изображение написанной от руки цифры 4, чёрные пиксели активируют связанные с ними нейроны, те активируют следующие слои, и так далее и далее, пока в итоге не загорится самый выход, отвечающий за четвёрку. результат достигнут.\\nв реальном программировании, естественно, никаких нейронов и связей не пишут, всё представляют матрицами и считают матричными произведениями, потому что нужна скорость. у меня есть два любимых видео, в которых весь описанный мной процесс наглядно объяснён на примере распознавания рукописных цифр. посмотрите, если хотите разобраться.\\nтакая сеть, где несколько слоёв и между ними связаны все нейроны, называется перцептроном (mlp) и считается самой простой архитектурой для новичков. в боевых задачах лично я никогда её не встречал.\\nкогда мы построили сеть, наша задача правильно расставить веса, чтобы нейроны реагировали на нужные сигналы. тут нужно вспомнить, что у нас же есть данные — примеры «входов» и правильных «выходов». будем показывать нейросети рисунок той же цифры 4 и говорить «подстрой свои веса так, чтобы на твоём выходе при таком входе всегда загоралась четвёрка».\\nсначала все веса просто расставлены случайно, мы показываем сети цифру, она выдаёт какой-то случайный ответ (весов-то нет), а мы сравниваем, насколько результат отличается от нужного нам. затем идём по сети в обратном направлении, от выходов ко входам, и говорим каждому нейрону — так, ты вот тут зачем-то активировался, из-за тебя всё пошло не так, давай ты будешь чуть меньше реагировать на вот эту связь и чуть больше на вон ту, ок?\\nчерез тысяч сто таких циклов «прогнали-проверили-наказали» есть надежда, что веса в сети откорректируются так, как мы хотели. научно этот подход называется backpropagation или «метод обратного распространения ошибки». забавно то, что чтобы открыть этот метод понадобилось двадцать лет. до него нейросети обучали как могли.\\nвторой мой любимый видос более подробно объясняет весь процесс, но всё так же просто, на пальцах.\\nхорошо обученная нейросеть могла притворяться любым алгоритмом из этой статьи, а зачастую даже работать точнее. такая универсальность сделала их дико популярными. наконец-то у нас есть архитектура человеческого мозга, говорили они, нужно просто собрать много слоёв и обучить их на любых данных, надеялись они. потом началась первая зима ии, потом оттепель, потом вторая волна разочарования.\\nоказалось, что на обучение сети с большим количеством слоёв требовались невозможные по тем временам мощности. сейчас любое игровое ведро с жифорсами превышает мощность тогдашнего датацентра. тогда даже надежды на это не было, и в нейросетях все сильно разочаровались.\\nпока лет десять назад не бомбанул диплёрнинг.\\nна английской википедии есть страничка timeline of machine learning, где хорошо видны всплески радости и волны отчаяния.\\nв 2012 году свёрточная нейросеть порвала всех в конкурсе imagenet, из-за чего в мире внезапно вспомнили о методах глубокого обучения, описанных еще в 90-х годах. теперь-то у нас есть видеокарты!\\nотличие глубокого обучения от классических нейросетей было в новых методах обучения, которые справлялись с большими размерами сетей. однако сегодня лишь теоретики разделяют, какое обучение можно считать глубоким, а какое не очень. мы же, как практики, используем популярные «глубокие» библиотеки типа keras, tensorflow и pytorch даже когда нам надо собрать мини-сетку на пять слоёв. просто потому что они удобнее всего того, что было раньше. мы называем это просто нейросетями.\\nрасскажу о двух главных на сегодняшний момент.\\nсвёрточные нейросети (cnn)\\nсвёрточные сети сейчас на пике популярности. они используются для поиска объектов на фото и видео, распознавания лиц, переноса стиля, генерации и дорисовки изображений, создания эффектов типа слоу-мо и улучшения качества фотографий. сегодня cnn применяют везде, где есть картинки или видео. даже в вашем айфоне несколько таких сетей смотрят на ваши голые фотографии, чтобы распознать объекты на них. если там, конечно, есть что распознавать хехехе.\\nкартинка выше — результат работы библиотеки detectron, которую facebook недавно заопенсорсил\\nпроблема с изображениями всегда была в том, что непонятно, как выделять на них признаки. текст можно разбить по предложениям, взять свойства слов из словарей. картинки же приходилось размечать руками, объясняя машине, где у котика на фотографии ушки, а где хвост. такой подход даже назвали «handcrafting признаков» и раньше все так и делали.\\nпроблем у ручного крафтинга много.\\nво-первых, если котик на фотографии прижал ушки или отвернулся — всё, нейросеть ничего не увидит.\\nво-вторых, попробуйте сами сейчас назвать хотя бы десять характерных признаков, отличающих котиков от других животных. я вот не смог. однако когда ночью мимо меня пробегает чёрное пятно, даже краем глаза я могу сказать котик это или крыса. потому что человек не смотрит только на форму ушей и количество лап — он оценивает объект по куче разных признаков, о которых сам даже не задумывается. а значит, не понимает и не может объяснить машине.\\nполучается, машине надо самой учиться искать эти признаки, составляя из каких-то базовых линий. будем делать так: для начала разделим изображение на блоки 8x8 пикселей и выберем какая линия доминирует в каждом — горизонтальная [-], вертикальная [|] или одна из диагональных [/]. могут и две, и три, так тоже бывает, мы не всегда точно уверены.\\nна выходе мы получим несколько массивов палочек, которые по сути являются простейшими признаками наличия очертаний объектов на картинке. по сути это тоже картинки, просто из палочек. значит мы можем вновь выбрать блок 8x8 и посмотреть уже, как эти палочки сочетаются друг с другом. а потом еще и еще.\\nтакая операция называется свёрткой, откуда и пошло название метода. свёртку можно представить как слой нейросети, ведь нейрон — абсолютно любая функция.\\nкогда мы прогоняем через нашу нейросеть кучу фотографий котов, она автоматически расставляет большие веса тем сочетаниям из палочек, которые увидела чаще всего. причём неважно, это прямая линия спины или сложный геометрический объект типа мордочки — что-то обязательно будет ярко активироваться.\\nна выходе же мы поставим простой перцептрон, который будет смотреть какие сочетания активировались и говорить кому они больше характерны — кошке или собаке.\\nкрасота идеи в том, что у нас получилась нейросеть, которая сама находит характерные признаки объектов. нам больше не надо отбирать их руками. мы можем сколько угодно кормить её изображениями любых объектов, просто нагуглив миллион картинок с ними — сеть сама составит карты признаков из палочек и научится определять что угодно.\\nпо этому поводу у меня даже есть несмешная шутка:\\nдай нейросети рыбу — она сможет определять рыбу до конца жизни. дай нейросети удочку — она сможет определять и удочку до конца жизни...\\nрекуррентные нейросети (rnn)\\nвторая по популярности архитектура на сегодняшний день. благодаря рекуррентным сетям у нас есть такие полезные вещи, как машинный перевод текстов (читайте мой пост об этом) и компьютерный синтез речи. на них решают все задачи, связанные с последовательностями — голосовые, текстовые или музыкальные.\\nпомните олдскульные голосовые синтезаторы типа microsoft sam из windows xp, который смешно произносил слова по буквам, пытаясь как-то склеить их между собой? а теперь посмотрите на amazon alexa или алису от яндекса — они сегодня не просто произносят слова без ошибок, они даже расставляют акценты в предложении!\\n нейросеть учится разговаривать\\nпотому что современные голосовые помощники обучают говорить не буквами, а фразами. но сразу заставить нейросеть целиком выдавать фразы не выйдет, ведь тогда ей надо будет запомнить все фразы в языке и её размер будет исполинским. тут на помощь приходит то, что текст, речь или музыка — это последовательности. каждое слово или звук — как бы самостоятельная единица, но которая зависит от предыдущих. когда эта связь теряется — получатся дабстеп.\\nдостаточно легко обучить сеть произносить отдельные слова или буквы. берём кучу размеченных на слова аудиофайлов и обучаем по входному слову выдавать нам последовательность сигналов, похожих на его произношение. сравниваем с оригиналом от диктора и пытаемся максимально приблизиться к идеалу. для такого подойдёт даже перцептрон.\\nвот только с последовательностью опять беда, ведь перцептрон не запоминает что он генерировал ранее. для него каждый запуск как в первый раз. появилась идея добавить к каждому нейрону память. так были придуманы рекуррентные сети, в которых каждый нейрон запоминал все свои предыдущие ответы и при следующем запуске использовал их как дополнительный вход. то есть нейрон мог сказать самому себе в будущем — эй, чувак, следующий звук должен звучать повыше, у нас тут гласная была (очень упрощенный пример).\\nбыла лишь одна проблема — когда каждый нейрон запоминал все прошлые результаты, в сети образовалось такое дикое количество входов, что обучить такое количество связей становилось нереально.\\nкогда нейросеть не умеет забывать — её нельзя обучить (у людей та же фигня).\\nсначала проблему решили в лоб — обрубили каждому нейрону память. но потом придумали в качестве этой «памяти» использовать специальные ячейки, похожие на память компьютера или регистры процессора. каждая ячейка позволяла записать в себя циферку, прочитать или сбросить — их назвали ячейки долгой и краткосрочной памяти (lstm).\\nкогда нейрону было нужно поставить себе напоминалку на будущее — он писал это в ячейку, когда наоборот вся история становилась ненужной (предложение, например, закончилось) — ячейки сбрасывались, оставляя только «долгосрочные» связи, как в классическом перцептроне. другими словами, сеть обучалась не только устанавливать текущие связи, но и ставить напоминалки.\\nпросто, но работает!\\n cnn + rnn = фейковый обама\\nозвученные тексты для обучения начали брать откуда угодно. даже базфид смог выгрузить видеозаписи выступлений обамы и весьма неплохо научить нейросеть разговаривать его голосом. на этом примере видно, что имитировать голос — достаточно простая задача для сегодняшних машин. с видео посложнее, но это пока.\\nпро архитектуры нейросетей можно говорить бесконечно. любознательных отправляю смотреть схему и читать статью neural network zoo, где собраны все типы нейронных сетей. есть и русская версия.\\nзаключение: когда на войну с машинами?\\nна вопрос «когда машины станут умнее нас и всех поработят?» я всегда отвечаю, что он заранее неправильный. в нём слишком много скрытых условий, который примаются как как данность.\\nвот мы говорим «станут умнее нас». значит мы подразумеваем, что существует некая единая шкала интеллекта, наверху которой находится человек, собаки пониже, а глупые голуби тусят в самом низу. получается человек должен превосходить нижестоящих животных во всём, так? а в жизни не так. средняя белка может помнить тысячу тайников в орешками, а я не могу вспомнить где ключи. получается интеллект — это набор разных навыков, а не единая измеримая величина? или просто запоминание орешков в него не входит? а убивание человеков входит?\\nну и самый интересный для меня вопрос — почему мы заранее считаем, что возможности человеческого мозга ограничены? в интернетах обожают рисовать графики, на которых технологический прогресс обозначен экспонентой, а возможности кожаных мешков константой. но так ли это?\\nвот давайте, прямо сейчас в уме умножьте 1680 на 950. да, знаю, вы даже пытаться не станете. но дай вам калькулятор, это займёт две секунды. значит ли это, что вы только что расширили возможности своего мозга с помощью калькулятора? можно ли продолжать их расширять другими машинами? я вот использую заметки на телефоне — значит ли это, что я расширяю свою память с помощью машины?\\nполучается, мы уже успешно расширяем способности нашего мозга с помощью машин. или нет?\\nподумайте. у меня всё.\\n\""]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjDfboiLnqpl","executionInfo":{"status":"ok","timestamp":1619379235551,"user_tz":-180,"elapsed":1467,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"f6603226-32af-4072-ce1b-4d36be6063c2"},"source":["sent_tokens[:5]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\nмашинное обучение для людей\\nразбираемся простыми словами\\nмашинное обучение — как секс в старших классах.',\n"," 'все говорят о нем по углам, единицы понимают, а занимается только препод.',\n"," 'статьи о машинном обучении делятся на два типа: это либо трёхтомники с формулами и теоремами, которые я ни разу не смог дочитать даже до середины, либо сказки об искусственном интеллекте, профессиях будущего и волшебных дата-саентистах.',\n"," 'зачем обучать машины\\nснова разберём на олегах.',\n"," 'предположим, олег хочет купить автомобиль и считает сколько денег ему нужно для этого накопить.']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yahyv942n9Xa","executionInfo":{"status":"ok","timestamp":1619379239126,"user_tz":-180,"elapsed":1388,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"bc6a7531-797a-4bfa-e595-1aa0a736a49e"},"source":["word_tokens[:5]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['машинное', 'обучение', 'для', 'людей', 'разбираемся']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"7BceOVEXoH7B","executionInfo":{"status":"ok","timestamp":1619379250762,"user_tz":-180,"elapsed":851,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["import pymorphy2\n","\n","lemmer = pymorphy2.MorphAnalyzer()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2sfjMPOI1NQ"},"source":["# For english\n","# from nltk.stem.wordnet import WordNetLemmatizer\n","# eng_lemmer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPNXz4Kfn_3n","executionInfo":{"status":"ok","timestamp":1619379254041,"user_tz":-180,"elapsed":845,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["def LemTokens(tokens):\n","    return [lemmer.parse(token)[0].normal_form for token in tokens]\n","    # return [eng_lemmer.lemmatize(token) for token in tokens]\n","    \n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","\n","def LemNormalize(text):\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLiNxI4-oyGo","executionInfo":{"status":"ok","timestamp":1619379265574,"user_tz":-180,"elapsed":886,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["import re \n","\n","# Тут нужно менять!\n","GREETING_INPUTS = (\"привет\", \"здравствуйте\", \"здорово\", \"чо каво\", \"хай\", \"салют\", \"хаюшки\", \"здоров\")\n","GREETING_RESPONSES = [\"привет\", \"хай\", \"*подмигнул*\", \"здарова\", \"здравствуй\", \"рад общению с тобой\"]\n","\n","def greeting(sentence):\n","    sentence = re.sub('[^А-Яа-яё ]', '', sentence)\n","    #sentence = re.sub('[^A-Za-z ]', '', sentence)\n","    for word in sentence.split():\n","        if word.lower() in GREETING_INPUTS:\n","            return random.choice(GREETING_RESPONSES)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"JcCVEtzopZS0","executionInfo":{"status":"ok","timestamp":1619379270778,"user_tz":-180,"elapsed":860,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.corpus import stopwords"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Kf3-Rx5peRP","executionInfo":{"status":"ok","timestamp":1619379277078,"user_tz":-180,"elapsed":854,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}}},"source":["def response(user_response):\n","    robo_response = ''\n","    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words(\"russian\"))\n","    # TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n","    tfidf = TfidfVec.fit_transform(sent_tokens)\n","    vals = cosine_similarity(tfidf[-1], tfidf)\n","    idx = vals.argsort()[0][-2]\n","    flat = vals.flatten()\n","    flat.sort()\n","    req_tfidf = flat[-2]\n","    if (req_tfidf == 0):\n","        robo_response = robo_response + \"Говори членораздельно и по-русски!\" # ТУТ можно менять :) \n","        return robo_response\n","    else:\n","        robo_response = robo_response + sent_tokens[idx]\n","        return robo_response"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UU2whSa7prjz","executionInfo":{"status":"ok","timestamp":1619379473438,"user_tz":-180,"elapsed":190768,"user":{"displayName":"Александр Апраксин","photoUrl":"","userId":"17818450637708548496"}},"outputId":"d2492685-fe73-4135-ee6d-c78a376e5ec7"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","flag = True\n","\n","# Тут тоже нужно менять!\n","print(\"R2D2: Меня зовут R2D2. Я буду отвечать на твои вопросы о машинном обучении.\\nЕсли ты захочешь прекратить общение, напиши \\'Бай\\'!\")\n","while flag:\n","    user_response = input()\n","    user_response = user_response.lower()\n","    if (user_response != 'бай'):\n","        if (user_response in ['спасибо', 'благодарю']): # И тут меняйте :)\n","            flag = False\n","            print(\"R2D2: Обращайся! ;)\") # Меняйте :) \n","        else:\n","            if (greeting(user_response) != None):\n","                print(\"R2D2: \" + greeting(user_response))\n","            else:\n","                sent_tokens.append(user_response)\n","                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n","                final_words = list(set(word_tokens))\n","                print(\"R2D2: \", response(user_response))\n","                sent_tokens.remove(user_response)\n","    else:\n","        flag = False\n","        print(\"R2D2: Увидимся! Ж8-))\") # Тут тоже нужно импровизировать"],"execution_count":15,"outputs":[{"output_type":"stream","text":["R2D2: Меня зовут R2D2. Я буду отвечать на твои вопросы о машинном обучении.\n","Если ты захочешь прекратить общение, напиши 'Бай'!\n","хай\n","R2D2: здравствуй\n","здоров\n","R2D2: хай\n","что такое беггинг\n","R2D2:  самый популярный пример беггинга — алгоритм random forest, беггинг на деревьях, который и нарисован на картинке.\n","что такое нейронка\n","R2D2:  Говори членораздельно и по-русски!\n","нейронная сеть\n","R2D2:  любознательных отправляю смотреть схему и читать статью neural network zoo, где собраны все типы нейронных сетей.\n","типы нейронных сетей\n","R2D2:  любознательных отправляю смотреть схему и читать статью neural network zoo, где собраны все типы нейронных сетей.\n","что такое ансамбли\n","R2D2:  потому есть три проверенных способа делать ансамбли.\n","что может робот\n","R2D2:  знания об окружающем мире такому роботу могут быть полезны, но чисто для справки.\n","что может искусственный интеллект\n","R2D2:  машинное обучение — это раздел искусственного интеллекта.\n","убить всех людей\n","R2D2:  машина может: предсказывать запоминать воспроизводить выбирать лучшее\n","машина не может: создавать новое резко поумнеть\tвыйти за рамки задачи убить всех людей\n","думаю потом нарисовать полноценную настенную карту со стрелочками и объяснениями, что где используется, если статья зайдёт.\n","бай\n","R2D2: Увидимся! Ж8-))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pT0iDnUiHa3n"},"source":["'''\n","Rasa\n","\n","sklearn\n","tensorflow\n","torch\n","transformers\n","mystem, pymorphy2, spacy\n","gensim\n","'''"],"execution_count":null,"outputs":[]}]}